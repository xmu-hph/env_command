# 总结

`Hopfield`网络典型的操作是内置一个对阵矩阵`W`，然后对输入的向量反复做矩阵映射，直到稳定。得到的就是输入向量在已知的向量几何中最接近的向量。缺点在于可存储的模式向量数量不多。

`Boltzmann Machine`网络典型的操作就是根据训练数据学习神经网络的参数，出发点是一个样本出现的概率是与 $f(x)$ 相关的，同时不考虑非线性，只考虑线性关系 $Wx$ ，假设说有隐变量，那么隐变量也通过线性关系合并 $Wx+Wv$ ，得到上述结果。

从玻尔兹曼机的代码来看，他的训练过程是：给定输入，估计概率进行采样，然后反估计概率及采样，然后再估计概率及采样。但是直接看代码会想不明白里面的参数更新公式。他的参数更新公式是需要数学理论推导的。数学理论的出发点就是样本出现的概率是与 $f(x)$ 相关的。背后是极大似然估计。什么模型的出发点是最小均方误差？`AE`的出发点是最小均方误差。`VAE`的出发点就是极大似然估计。从这个角度来说，玻尔兹曼机还真是一个方向的开山鼻祖。算是概率学派的先锋了。

关键是这个导数的形式也非常神奇，跟二分类的`logistic`回归问题的解也非常类似。一个是监督学习，一个是无监督学习。这里面的结果惊人的相似。

参考：[受限玻尔兹曼机（RBM）原理总结](https://www.cnblogs.com/pinard/p/6530523.html)

但是现在回头看`Hopfield`网络，反而有点看不懂了。`Hopfield`网络的代码非常容易看懂，就是对向量不断进行矩阵乘法，直到收敛。但是有个问题，神经网络中投影矩阵的确定的计算公式有些简单。不知道什么计算公式。

所以说人工智能的表述形态发生了很大的变化。机器学习时代，我们是有监督的形式，可以定义从输入到结果的映射，然后根据极大似然估计或者最小二乘误差计算损失的表达式，然后求导更新参数。

在深度学习时代，更多的时候是无监督的形式或隐监督的形式，监督的标签未知，更多的是根据样本发生的情况来求极大似然估计，计算表达式求导数然后更新参数。

在大语言模型时代，大部分时候不讨论是极大似然估计还是最小二乘误差，反而是关注预测结果与真实结果之间的距离--交叉熵或者均方误差，似乎直接通过非线性神经网络（`cnn`、`rnn`、`transformer`、`mamba`）得到的隐藏变量然后接个`dnn`非线性层然后就可以得到任何想要的结果，通过结果之间的误差交叉熵或者均方误差就可以对参数进行优化。这很不正常。拿`bert`举例，输入数据是文本，但是先分词化，然后再`embedding`化，然后再用注意力机制做多层非线性投影，然后对最后一层的`embedding`接个`dnn`用词的交叉熵来训练模型。最后的使用环节就是替换一下最后的`dnn`层进行新的微调。但是很奇怪似乎不关注数学理论，或者说数学理论框架就是玻尔兹曼机这一套，不过就是把其中的函数表达式从 $f(x)=wx+b$ 变成了看不懂的神经网络层的堆叠。这些层堆叠以后的函数表达式已经无法表示，同时也不能用手工解析的方式得到解析解。只能通过`backward`计算。

# 备注
记录看书过程中的灵感