# 总结

`Hopfield`网络典型的操作是内置一个对阵矩阵`W`，然后对输入的向量反复做矩阵映射，直到稳定。得到的就是输入向量在已知的向量几何中最接近的向量。缺点在于可存储的模式向量数量不多。这个网络的典型思想就是通过函数变换成另一个向量。是人工智能中的一大思路出发点。

`Boltzmann Machine`网络典型的操作就是根据训练数据学习神经网络的参数，出发点是一个样本出现的概率是与 $f(x)$ 相关的，同时不考虑非线性，只考虑线性关系 $Wx$ ，假设说有隐变量，那么隐变量也通过线性关系合并 $Wx+Wv$ ，得到上述结果。

从玻尔兹曼机的代码来看，他的训练过程是：给定输入，估计概率进行采样，然后反估计概率及采样，然后再估计概率及采样。但是直接看代码会想不明白里面的参数更新公式。他的参数更新公式是需要数学理论推导的。数学理论的出发点就是样本出现的概率是与 $f(x)$ 相关的。背后是极大似然估计。什么模型的出发点是最小均方误差？`AE`的出发点是最小均方误差。`VAE`的出发点就是极大似然估计。从这个角度来说，玻尔兹曼机还真是一个方向的开山鼻祖。算是概率学派的先锋了。

关键是这个导数的形式也非常神奇，跟二分类的`logistic`回归问题的解也非常类似。一个是监督学习，一个是无监督学习。这里面的结果惊人的相似。

参考：[受限玻尔兹曼机（RBM）原理总结](https://www.cnblogs.com/pinard/p/6530523.html)

但是现在回头看`Hopfield`网络，反而有点看不懂了。`Hopfield`网络


















# 备注
记录看书过程中的灵感